<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Fox and the flu</title>
    <link>https://sjfox.github.io/post/index.xml</link>
    <description>Recent content in Posts on Fox and the flu</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2016. All rights reserved.</copyright>
    <lastBuildDate>Thu, 11 Jan 2018 15:28:21 -0600</lastBuildDate>
    <atom:link href="https://sjfox.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Visualizing flight data with ggplot2</title>
      <link>https://sjfox.github.io/post/world_map_flights/</link>
      <pubDate>Thu, 11 Jan 2018 15:28:21 -0600</pubDate>
      
      <guid>https://sjfox.github.io/post/world_map_flights/</guid>
      <description>&lt;p&gt;Between travel and finishing up a couple of projects the past two semesters, I wasn’t able to blog as much as I’d like. Rather than jumping straing into research after the holidays, it seemed like a nice time to blog about my favorite figure from the past semester. It was the striking image associated with &lt;a href=&#34;http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005749&#34;&gt;this paper&lt;/a&gt;, though I can’t seem to find a link for it online, so here it is:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://sjfox.github.io/img/striking_image_world_map.png&#34; title=&#34;Hypothetical flu spreading visualization&#34; alt=&#34;flight image&#34; /&gt; Anyway to begin you’re going to need to download the flight and airport data from &lt;a href=&#34;https://openflights.org/data.html&#34;&gt;here&lt;/a&gt;, which is a really amazing sight aggregating flight data and making it available for free!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)

airports &amp;lt;- read_csv(&amp;quot;data/flights/airports.csv&amp;quot;, col_names = c(&amp;quot;id&amp;quot;, &amp;quot;name&amp;quot;, 
    &amp;quot;city&amp;quot;, &amp;quot;country&amp;quot;, &amp;quot;iata&amp;quot;, &amp;quot;icao&amp;quot;, &amp;quot;lat&amp;quot;, &amp;quot;long&amp;quot;, &amp;quot;alt&amp;quot;, &amp;quot;tz&amp;quot;, &amp;quot;dst&amp;quot;, &amp;quot;tz2&amp;quot;, 
    &amp;quot;type&amp;quot;, &amp;quot;source&amp;quot;), na = c(&amp;quot;\\N&amp;quot;))
head(airports)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 14
##      id                                        name         city
##   &amp;lt;int&amp;gt;                                       &amp;lt;chr&amp;gt;        &amp;lt;chr&amp;gt;
## 1     1                              Goroka Airport       Goroka
## 2     2                              Madang Airport       Madang
## 3     3                Mount Hagen Kagamuga Airport  Mount Hagen
## 4     4                              Nadzab Airport       Nadzab
## 5     5 Port Moresby Jacksons International Airport Port Moresby
## 6     6                 Wewak International Airport        Wewak
## # ... with 11 more variables: country &amp;lt;chr&amp;gt;, iata &amp;lt;chr&amp;gt;, icao &amp;lt;chr&amp;gt;,
## #   lat &amp;lt;dbl&amp;gt;, long &amp;lt;dbl&amp;gt;, alt &amp;lt;int&amp;gt;, tz &amp;lt;dbl&amp;gt;, dst &amp;lt;chr&amp;gt;, tz2 &amp;lt;chr&amp;gt;,
## #   type &amp;lt;chr&amp;gt;, source &amp;lt;chr&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# flights &amp;lt;- read_csv(&amp;#39;data/airports2/routes.csv&amp;#39;, col_names = c(&amp;#39;airline&amp;#39;,
# &amp;#39;id&amp;#39;, &amp;#39;src_airport&amp;#39;, &amp;#39;src_airport_id&amp;#39;, &amp;#39;dest_airport&amp;#39;, &amp;#39;dest_airport_id&amp;#39;,
# &amp;#39;codeshare&amp;#39;, &amp;#39;stops&amp;#39;, &amp;#39;equip&amp;#39;), na = c(&amp;#39;\\N&amp;#39;)) airports &amp;lt;- airports %&amp;gt;%
# select(id, lat, long) flights %&amp;gt;% select(src_airport_id, dest_airport_id)
# %&amp;gt;% filter(!is.na(src_airport_id) &amp;amp; !is.na(dest_airport_id)) %&amp;gt;%
# left_join(airports, by = c(&amp;#39;src_airport_id&amp;#39; = &amp;#39;id&amp;#39;)) %&amp;gt;%
# rename(src_lat=lat, src_long=long) %&amp;gt;% left_join(airports, by =
# c(&amp;#39;dest_airport_id&amp;#39; = &amp;#39;id&amp;#39;)) %&amp;gt;% rename(dest_lat=lat, dest_long=long) %&amp;gt;%
# filter(src_airport_id != dest_airport_id) %&amp;gt;% mutate(ordered_pair =
# if_else(src_airport_id &amp;gt; dest_airport_id,
# paste0(dest_airport_id,src_airport_id),
# paste0(src_airport_id,dest_airport_id))) %&amp;gt;%
# filter(!duplicated(ordered_pair)) %&amp;gt;% mutate(is_europe = if_else(dest_lat
# &amp;lt; 60 &amp;amp; dest_lat &amp;gt; 25 &amp;amp; src_lat &amp;lt; 60 &amp;amp; src_lat &amp;gt; 10 &amp;amp; dest_long &amp;lt; 50 &amp;amp;
# dest_long &amp;gt; -15 &amp;amp; src_long &amp;lt; 50 &amp;amp; src_long &amp;gt; -15, TRUE,FALSE)) %&amp;gt;%
# filter(!is.na(is_europe)) -&amp;gt; all_flights data &amp;lt;- map_data(&amp;#39;world&amp;#39;) plot_bg
# &amp;lt;- data %&amp;gt;% filter(`region` != &amp;#39;Antarctica&amp;#39;) %&amp;gt;% ggplot(aes(long, lat,
# group = group)) + geom_polygon(fill=&amp;#39;black&amp;#39;, color = &amp;#39;black&amp;#39;, size=0.1) +
# theme_void() + theme(plot.background=element_rect(fill=&amp;#39;black&amp;#39;)) world_map
# &amp;lt;- plot_bg + geom_polygon(fill=&amp;#39;black&amp;#39;, color = &amp;#39;white&amp;#39;, size=0.15)
# only_curves &amp;lt;- plot_bg + geom_curve(data = all_flights, aes(x = src_long,
# xend = dest_long, y = src_lat, yend = dest_lat, size=is_europe,
# alpha=is_europe), color = &amp;#39;white&amp;#39;, inherit.aes = FALSE) +
# scale_size_manual(values = c(0.05, 0.01) ) + scale_alpha_manual(values =
# c(0.25, 0.25) ) + theme(legend.position=&amp;#39;none&amp;#39;,
# plot.background=element_rect(fill=&amp;#39;black&amp;#39;))&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>A pesky piping bug with RStudio and the tidyverse</title>
      <link>https://sjfox.github.io/post/piping_bug/</link>
      <pubDate>Thu, 21 Sep 2017 13:25:32 -0500</pubDate>
      
      <guid>https://sjfox.github.io/post/piping_bug/</guid>
      <description>&lt;p&gt;Today I encountered an error in the &lt;code&gt;tidyverse&lt;/code&gt; that took me a while to figure out, so I wanted to document it for others. I’m also not sure where this error originates, so I wasn’t positive which package to post it in (or with RStudio) - I’d be happy to submit a bug report if anyone can point me in the right direction :)&lt;/p&gt;
&lt;div id=&#34;the-scenario&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Scenario&lt;/h2&gt;
&lt;p&gt;You have a data frame you want to alter using piping, something like this.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)

data_frame(x=1:10)  %&amp;gt;%  
  mutate(x2 = x*2,  
         x3 = x*3) %&amp;gt;%  
  filter(x&amp;gt;5)   &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 5 x 3
##       x    x2    x3
##   &amp;lt;int&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1     6    12    18
## 2     7    14    21
## 3     8    16    24
## 4     9    18    27
## 5    10    20    30&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Those lines work no matter how you run them. However, let’s say we no longer want to make the &lt;code&gt;x2&lt;/code&gt; column, so we comment out the line to potentially save it for later if we change our minds:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data_frame(x=1:10)  %&amp;gt;%  
  mutate(# x2 = x*2,  
         x3 = x*3) %&amp;gt;%  
  filter(x&amp;gt;5)   &lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;the-bug&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Bug&lt;/h2&gt;
&lt;p&gt;The whole code runs perfectly if you select the whole chunk, or simply have your cursor on either of the first two lines before running with cmd + enter / ctrl + enter. However, if you put your cursor on the third or fourth lines, you get weird multiple line errors:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt; x3 = x*3) %&amp;gt;%  
Error: unexpected &amp;#39;)&amp;#39; in &amp;quot;x3 = x*3)&amp;quot;
&amp;gt;   filter(x&amp;gt;5)
Error in filter(x &amp;gt; 5) : object &amp;#39;x&amp;#39; not found&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The error seems to to be that RStudio (or maybe one of the packages) interpret the &lt;code&gt;#&lt;/code&gt; following &lt;code&gt;mutate&lt;/code&gt; as breaking the chunk somehow, so it doesn’t run the whole chunk. I was surprised at this, though, because the following code works perfectly fine, regardless of how you try to run it (even if you try from cursor on the last two lines):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data_frame(x=1:10)  %&amp;gt;%  
  mutate(x1 = x*1,
         # x2 = x*2,  
         x3 = x*3) %&amp;gt;%  
  filter(x&amp;gt;5)  &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 5 x 3
##       x    x1    x3
##   &amp;lt;int&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1     6     6    18
## 2     7     7    21
## 3     8     8    24
## 4     9     9    27
## 5    10    10    30&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So it appears it is dependent on the &lt;code&gt;#&lt;/code&gt; being directly after the function, maybe? I haven’t gone through all iterations to more fully understand it.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;As far as bugs go, this is fairly inconsequential. However, it took me a while to debug the error on a somewhat complicated 15 line pipe with ggplot, because of the inconsistency in the error R spat out as well as the innocuousness of the causitive code. Do you get the same errors as I do? If so, should it be fixed, or is there a purpose I’m not seeing?&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>You should make an R package for your paper</title>
      <link>https://sjfox.github.io/post/2017-05-04-rtzikvrisk_primer/</link>
      <pubDate>Wed, 10 May 2017 10:37:49 -0500</pubDate>
      
      <guid>https://sjfox.github.io/post/2017-05-04-rtzikvrisk_primer/</guid>
      <description>&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;I had wanted to get into R package creation for a while now, but finally got a chance to do so for my lab’s recent Zika paper, &lt;a href=&#34;https://bmcinfectdis.biomedcentral.com/articles/10.1186/s12879-017-2394-9&#34;&gt;Assessing Real-time Zika Risk in the United States&lt;/a&gt;. If you’re interested in learning about the content from that paper you can check out the blogpost hosted on the &lt;a href=&#34;https://blogs.biomedcentral.com/bmcseriesblog/2017/05/05/assessing-real-time-zika-risk-in-the-united-states/&#34;&gt;BMC Infectious Diseases blog&lt;/a&gt;. I’ve also given an introduction to the package on the &lt;a href=&#34;http://htmlpreview.github.io/?https://github.com/sjfox/rtZIKVrisk/blob/master/inst/doc/introduction.html&#34;&gt;rtZIKVrisk github&lt;/a&gt;. Here I wanted to talk a bit about some of the advantages of putting your papers into packages (both for yourself and other researchers).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;what-goes-into-an-r-package&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;What goes into an R package&lt;/h1&gt;
&lt;p&gt;R packages are made up of three main things: (1) data, (2) functions, and (3) documentation. Combining them into a single, self-contained bundle, means that all of the computational work from a project can be found in one place, downloaded and installed simply, and then run with a freely availible software anyone can obtain. As an added bonus you can even add a vignette to the package, which you can use to illustrate example analyses or a sample pipeline.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;why-you-should-turn-papers-into-packages&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Why you should turn papers into packages&lt;/h1&gt;
&lt;p&gt;So aside from the obvious benefit, why should you, a scientist who has limited time consider turning your paper into an R package?&lt;/p&gt;
&lt;div id=&#34;it-doesnt-take-that-much-time&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;It doesn’t take that much time&lt;/h2&gt;
&lt;p&gt;I started with &lt;a href=&#34;https://hilaryparker.com/2014/04/29/writing-an-r-package-from-scratch/&#34;&gt;Hilary Parker’s introduction&lt;/a&gt;, and then looked at &lt;a href=&#34;http://r-pkgs.had.co.nz/&#34;&gt;Hadley Wickham’s&lt;/a&gt; more comprehensive book when I needed it. All-in-all, it took me about a solid day and a half to take my existing code and put it into the documented and fully functional package. In the scheme of things that’s not that much time, and it gets quicker when you’ve done it once. I’ve reduced the time burden even further, because I’ve now started coding my current projects as R packages, so there’s no need to convert everything over once it’s finished (I haven’t fully mastered this skill, though I’ll likely write a blogpost about that later on).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;foster-open-science&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Foster open science&lt;/h2&gt;
&lt;p&gt;I mentioned earlier that R packages keep your data in a self-contained bundle, and I think this is one of the best reasons to turn a paper into a package. Making a package allows anyone to reproduce your results, see your code, and more fully understand your analysis. It also ensures that the necessary R and package versions are all documented. With a simple R command anyone can download &lt;code&gt;rtZIKVrisk&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;    install.packages(&amp;quot;devtools&amp;quot;)
    devtools::install_github(&amp;quot;sjfox/rtZIKVrisk&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From there a few lines is all that it takes to recreate any of our figures,&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;    library(rtZIKVrisk)
    plot_fig2()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://sjfox.github.io/post/2017-05-04-rtzikvrisk_primer_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;700px&#34; /&gt;&lt;/p&gt;
&lt;p&gt;or begin simulating epidemics in Texas counties.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;  set.seed(808)
  sim_parms &amp;lt;- zika_def_parms()
  outbreak_sim &amp;lt;- run_n_zika_sims(num_reps = 100, sim_parms)
  plot_final_sizes(outbreak_sim)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://sjfox.github.io/post/2017-05-04-rtzikvrisk_primer_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;  plot_zika_outbreaks(outbreak_sim)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://sjfox.github.io/post/2017-05-04-rtzikvrisk_primer_files/figure-html/unnamed-chunk-3-2.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;p&gt;If they have any issues running the code, they can use the standard help functions from within R to look at the function documentation and examples.&lt;/p&gt;
&lt;p&gt;I think this is really cool, but also realize that few people will ever use the simulations or analysis functions from this package. At the very least people can now look at our branching process epidemic simulation code &lt;code&gt;run_zika_sim()&lt;/code&gt;, and learn how to implement one for their own purposes. I know that I’ve learned the most about methods from papers when I can look at both the description within the paper and the actual code as well.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;improve-your-coding&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Improve your coding&lt;/h2&gt;
&lt;p&gt;We started this project in April of 2016 in a rush, trying to get the project pushed out in a couple of weeks. That meant that our project repository on github was (and still is) a mess. Collaborators all had their own scripts, and it was hard to keep track of all of the changes.&lt;/p&gt;
&lt;p&gt;Turning those scripts into a package made me think hard about the functions from those scripts that we were actually using and putting them into a reasonable format that was organized. I broke the code up into data munging, simulation, analysis, and plotting functions, which could then be self contained pieces. I now know where to look for functions for each stage of the analysis, and can also see the documentation with the simple R search &lt;code&gt;?run_zika_sim&lt;/code&gt; (being forced to make documentation for your functions can be annoying, but it is extremely useful in the end). Overall, I was able to make a much more permanent location for the materials from the paper, and make it more accessible to myself and others in the future – knowing that others will see your code definitely helps force you to improve.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;get-people-interested-in-your-project&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Get people interested in your project&lt;/h2&gt;
&lt;p&gt;Once you have your package framework in place, you sometimes think of interesting ideas not entirely related to your specific paper that might appeal to others. For example, I implemented some functionality to query our database of Texas county information, and run example outbreaks in a specific county. Here’s how I would do that in my hometown of Austin.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;travis_parms &amp;lt;- get_county_parms(&amp;quot;travis&amp;quot;)
travis_sims &amp;lt;- run_n_zika_sims(num_reps = 1000, sim_parms)
plot_final_sizes(travis_sims)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://sjfox.github.io/post/2017-05-04-rtzikvrisk_primer_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Plot the epidemic probablity as a function of reported cases based on simulations
travis_epi_prob &amp;lt;- get_epidemic_prob_by_d(travis_sims, prev_threshold = 10, cum_threshold = 100)
plot_epi_prob(travis_epi_prob)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://sjfox.github.io/post/2017-05-04-rtzikvrisk_primer_files/figure-html/unnamed-chunk-4-2.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This was a cool feature that I thought (and still hope) that the people impacted by our study might be interested in seeing and exploring for themselves.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;easily-regenerate-figures&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Easily regenerate figures&lt;/h2&gt;
&lt;p&gt;Remember the last time you needed to get figures together for a presentation or poster? If you’re like me, you probably wanted to tweak things a bit from your manuscript figure. Even if you wanted to make a simple change to the text size, it probably involved digging through old R scripts, wading through code to find the right figure plots, and then praying that the code still runs. The largest benefit of having the R package for me so far, has been for easily making the figures, and if you’re going to do anything for your paper I would suggest doing this.&lt;/p&gt;
&lt;p&gt;All of the data contained within the figures can be packaged in with the plotting functions. That means that outputting the figures is quick and foolproof, because you don’t need to regenerate the data (something I know &lt;a href=&#34;https://twitter.com/ClausWilke&#34;&gt;Claus Wilke&lt;/a&gt; has advised in a lab meeting at some point in time). I also included the ability to plot only single panels from the multi-panel plots, which will help make presentation slides without having to clip out the letters or worry about resolution. You could further change the text size and ratios using the &lt;code&gt;save_plot()&lt;/code&gt; function from within the &lt;a href=&#34;https://cran.r-project.org/web/packages/cowplot/vignettes/introduction.html&#34;&gt;cowplot package&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;    plot_fig2(panels = &amp;quot;a&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://sjfox.github.io/post/2017-05-04-rtzikvrisk_primer_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;480&#34; /&gt;&lt;/p&gt;
&lt;p&gt;or if you’d like to say remove one of the layers, you can customize it by looking at the layers of the ggplot and removing the layers you don’t want to be plotted. So let’s remove the points and text from the previous plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;    p &amp;lt;- plot_fig2(panels = &amp;quot;a&amp;quot;)
    p$layers&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1]]
## mapping: group = group, fill = log(import_prob) 
## geom_polygon: na.rm = FALSE
## stat_identity: na.rm = FALSE
## position_identity 
## 
## [[2]]
## mapping: x = lon, y = lat 
## geom_point: na.rm = FALSE
## stat_identity: na.rm = FALSE
## position_identity 
## 
## [[3]]
## mapping: x = lon, y = lat, label = Name 
## geom_text_repel: parse = FALSE, na.rm = FALSE, box.padding = 0.25, point.padding = 1e-06, segment.colour = black, segment.size = 0.5, segment.alpha = NULL, min.segment.length = 0.5, arrow = NULL, force = 0.75, max.iter = 2000, nudge_x = 0, nudge_y = 0
## stat_identity: na.rm = FALSE
## position_identity&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;    # Remove the points and text
    p$layers[[2]] &amp;lt;- p$layers[[3]] &amp;lt;- NULL
    p&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://sjfox.github.io/post/2017-05-04-rtzikvrisk_primer_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;480&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This is a great technique for making consecutive presentation slides that add features as you go to help walk people through your results.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;why-you-shouldnt-turn-papers-into-packages&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Why you shouldn’t turn papers into packages&lt;/h1&gt;
&lt;p&gt;I don’t think there’s a one-size-fits-all solution for how to make your code and analyses available for everyone to use, so here are a few cases where it might not make sense to make an R package.&lt;/p&gt;
&lt;div id=&#34;you-already-have-a-robust-way-for-sharing-your-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;You already have a robust way for sharing your analysis&lt;/h2&gt;
&lt;p&gt;I personally like the self-contained nature of R packages, but I’m sure there are many legitimate ways to make that happen. I know many people that make a single github repository with a well documented README that seems to work pretty well. I think &lt;a href=&#34;https://github.com/TAlexPerkins/Zika_nmicrobiol_2016&#34;&gt;this well-documented repo&lt;/a&gt; from Dr. Alex Perkins is a great example of doing just that.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;your-analysis-isnt-self-contained-within-r&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Your analysis isn’t self-contained within R&lt;/h2&gt;
&lt;p&gt;One of my other projects uses C++ to run simulations, and then I do the analysis in R. In my mind it’s more confusing to have an R package for those analysis scripts, because the simulation code would need to be documented elswhere. I think for projects like that it makes more sense to have a github repository or something similar that can contain everything with a well documented README. In the future, though, I’d like to integrate that C++ code with R through &lt;code&gt;RCpp&lt;/code&gt;, to be able to make it into a self-contained R package.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;youre-in-too-deep-on-your-current-project&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;You’re in too deep on your current project&lt;/h2&gt;
&lt;p&gt;I get it, things are crazy, you’re code is a mess, and you don’t have time to turn your current analysis into an R package (though I hope you’re still making the code available in some format). Please consider just making the R package with the figure plotting functions I discussed above though. Trust me, you won’t regret it! As for the whole analysis in a package, just keep it in mind for your next project. It’s definitely more time efficient if you start your analysis with the R package in mind compared to converting it after the fact.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;I’m curious to hear other people’s opinions on this, so let me what you think &lt;a href=&#34;https://twitter.com/foxandtheflu&#34;&gt;@foxandtheflu&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Simple trick to speed up ODE integration in R</title>
      <link>https://sjfox.github.io/post/2017-04-19-timesteps-lsoda-and-epidemiological-models/</link>
      <pubDate>Mon, 24 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>https://sjfox.github.io/post/2017-04-19-timesteps-lsoda-and-epidemiological-models/</guid>
      <description>&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;I recently was doing model fitting on a ton of simulations, and needed to figure out a way to speed things up. My first instinct was to get out of the &lt;code&gt;R&lt;/code&gt; environment and write &lt;code&gt;CSnippets&lt;/code&gt; for the &lt;code&gt;pomp&lt;/code&gt; package (more on this in a later blog), or to use &lt;code&gt;RCpp&lt;/code&gt;, but I used the &lt;code&gt;profvis&lt;/code&gt; package to help diagnose the speed issues, and found a really simple change that can save a ton of time depending on your model.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The model&lt;/h2&gt;
&lt;p&gt;Let’s start by making a simple SIR model. This model has susceptible, infectious, and recovered individuals, and the ode will follow the number of individuals in each class over the course of the epidemic. &lt;code&gt;beta&lt;/code&gt; and &lt;code&gt;gamma&lt;/code&gt; will be parameters governing the transmission and recovery rates of the individuals. We’ll make two forms of the model: one that is more legible and utilizes the &lt;code&gt;with&lt;/code&gt; function, and another one that is slightly less legible but accesses the vectors directly. I’ll also create a function that can run either of the different models, and return a dataframe as a result.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sir_simple &amp;lt;- function(t, x, params) {
    with(c(as.list(params), as.list(x)), {
        dS &amp;lt;- -beta * S * I
        dI &amp;lt;- beta * S * I - gamma * I
        dR &amp;lt;- gamma * I
        dx &amp;lt;- c(dS, dI, dR)
        list(dx)
    })
}

sir &amp;lt;- function(t, x, params) {
    dS &amp;lt;- -params[&amp;quot;beta&amp;quot;] * x[&amp;quot;S&amp;quot;] * x[&amp;quot;I&amp;quot;]
    dI &amp;lt;- params[&amp;quot;beta&amp;quot;] * x[&amp;quot;S&amp;quot;] * x[&amp;quot;I&amp;quot;] - params[&amp;quot;gamma&amp;quot;] * x[&amp;quot;I&amp;quot;]
    dR &amp;lt;- params[&amp;quot;gamma&amp;quot;] * x[&amp;quot;I&amp;quot;]
    dx &amp;lt;- c(dS, dI, dR)
    list(dx)
}


run_sir &amp;lt;- function(init_states, times, params, sir_func) {
    as.data.frame(lsoda(y = init_states, times = times, func = sir_func, parms = params))
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;single-run&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Single run&lt;/h2&gt;
&lt;p&gt;Now let’s initialize and run a model. We will use the &lt;code&gt;deSolve&lt;/code&gt; package for running the ODEs, the &lt;code&gt;tidyverse&lt;/code&gt; to handle data manipulations, and &lt;code&gt;cowplot&lt;/code&gt;/&lt;code&gt;ggplot2&lt;/code&gt; for visualizing. We will run the model with &lt;span class=&#34;math inline&#34;&gt;\(\beta=0.5\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\gamma=0.25\)&lt;/span&gt;, so the disease will have an &lt;span class=&#34;math inline&#34;&gt;\(R_0 = \frac{\beta}{\gamma}=2\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(deSolve)
library(tidyverse)
library(cowplot)

times &amp;lt;- seq(0, 100, by = 1)
params &amp;lt;- c(beta = 0.5, gamma = 0.25)
init_states &amp;lt;- c(S = 9999/10000, I = 1/10000, R = 0/10000)

epi_dat &amp;lt;- run_sir(init_states, times, params, sir_simple)

epi_dat %&amp;gt;% head()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   time         S            I            R
## 1    0 0.9999000 0.0001000000 0.000000e+00
## 2    1 0.9998429 0.0001285559 2.856343e-05
## 3    2 0.9997695 0.0001652465 6.526861e-05
## 4    3 0.9996752 0.0002123925 1.124413e-04
## 5    4 0.9995548 0.0002725541 1.726493e-04
## 6    5 0.9994000 0.0003499144 2.500909e-04&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;epi_dat %&amp;gt;% gather(state, value, S:R) %&amp;gt;% ggplot(aes(time, value, color = state)) + 
    geom_line(size = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://sjfox.github.io/post/2017-04-19-timesteps-lsoda-and-epidemiological-models_files/figure-html/run_sim-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Let’s also confirm that both models give the same result.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;epi_dat_base &amp;lt;- run_sir(init_states, times, params, sir)

all_equal(epi_dat, epi_dat_base)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;comparing-the-two-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Comparing the two models&lt;/h2&gt;
&lt;p&gt;This model is extremely simplistic and runs very quickly on my machine, but let’s use the &lt;code&gt;microbenchmark&lt;/code&gt; package to compare the run times of the two different models.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(microbenchmark)
benchmark_std &amp;lt;- microbenchmark(more_readable = run_sir(init_states, times, 
    params, sir_simple), not_readable = run_sir(init_states, times, params, 
    sir), times = 100)
summary(benchmark_std)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            expr      min       lq     mean   median       uq      max
## 1 more_readable 5.326989 6.094288 7.564684 6.522981 7.230016 66.76668
## 2  not_readable 3.222771 3.545130 4.183180 3.869331 4.382511 10.11805
##   neval
## 1   100
## 2   100&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;autoplot(benchmark_std) + scale_y_continuous(trans = &amp;quot;identity&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://sjfox.github.io/post/2017-04-19-timesteps-lsoda-and-epidemiological-models_files/figure-html/bechmarking-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusions&lt;/h2&gt;
&lt;p&gt;Looking at the results the model without using the &lt;code&gt;with&lt;/code&gt; statement runs about twice as fast as using the &lt;code&gt;with&lt;/code&gt; statement. This may not seem like a big enough of a difference to matter, but shaving off seconds from the ODE simulation can reap large speed benefits when you are using fitting procedures which run many simulations to optimize parameters. The model I was fitting was much more complex, and I received an order of magnitude speed increase by removing the &lt;code&gt;with&lt;/code&gt; statement from the ODE – I think I received a larger effect than the simple SIR model here, because I had many more parameters, and had other outside function calls within the ODE, though I’m not sure which one contributed stronger to the effect.&lt;/p&gt;
&lt;p&gt;This is a simple change to the model that can help shave off some time, so I would recommend trying it out with your simulations and see how much (if at all) it speeds things up. That being said, it’s always a tradeoff between code readability and speed, so if you’re not running into time issues you may want to stick with the more readable version using &lt;code&gt;with&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Do you have any other speedup tips for running and fitting ODEs in R or any questions about what I’ve done here? Would love to hear from you, so send me a message &lt;a href=&#34;https://twitter.com/foxandtheflu&#34;&gt;@foxandtheflu&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Bootstrapping Time Series Data</title>
      <link>https://sjfox.github.io/post/2015-07-23-r-rmarkdown/</link>
      <pubDate>Sat, 23 Jul 2016 21:13:14 -0500</pubDate>
      
      <guid>https://sjfox.github.io/post/2015-07-23-r-rmarkdown/</guid>
      <description>&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;This demonstration is part of a requirement for my statistical consulting class at UT Austin. I will go through the basics of bootstrapping time series data using three different resampling methods.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Fixed Block Sampling&lt;/li&gt;
&lt;li&gt;Stationary Block Sampling&lt;/li&gt;
&lt;li&gt;Model-based resampling&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;packages-used&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Packages Used&lt;/h1&gt;
&lt;p&gt;For this demonstration I will use the following packages: The &lt;code&gt;boot&lt;/code&gt; package is the workhorse behind the bootstrapping methods, but the &lt;code&gt;forecast&lt;/code&gt; method is used for the time series modeling. &lt;code&gt;tidyverse&lt;/code&gt;, &lt;code&gt;cowplot&lt;/code&gt;, and &lt;code&gt;lubridate&lt;/code&gt; are all packages used for cleaning the data and presenting the results at the end, so aren’t as necessary depending on your preferred methods.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(boot)
library(forecast)
library(tidyverse)
library(cowplot)
library(lubridate)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;question-of-interest&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Question of interest&lt;/h1&gt;
&lt;p&gt;We will be investigating the Rio Negro river level in the &lt;code&gt;manaus&lt;/code&gt; dataset. We are interested in developing an AR(p) model to describe the river level through time. However, we are not interested in selecting the &lt;code&gt;p&lt;/code&gt; that is the best for the data, but rather understanding the full distribution of &lt;code&gt;p&lt;/code&gt; for these data, which is where the bootstrapping will come into play.&lt;/p&gt;
&lt;p&gt;Always a good idea to see what your data look like first:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(manaus)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://sjfox.github.io/post/2015-07-23-r-rmarkdown_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;creating-a-function-for-obtaining-a-test-statistic&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Creating a function for obtaining a test statistic&lt;/h1&gt;
&lt;p&gt;The &lt;code&gt;boot&lt;/code&gt; package requires a function that returns your statistic of interest for any data supplied. We will create a function that finds the maximum likelihood order (p) for a time series that is supplied as &lt;code&gt;tsb&lt;/code&gt;. We return the best fit order, the mean of the time-series, and then the whole time series data, as a means to extract this information later. Really you would only need to return the statistic itself depending on your goals.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;manaus_fun &amp;lt;- function(tsb) {
    ar.fit &amp;lt;- auto.arima(tsb, max.p = 25, max.d = 0, max.q = 0, max.P = 0, max.Q = 0, 
        max.D = 0, ic = &amp;quot;aic&amp;quot;, max.order = 25, seasonal = FALSE)
    c(ar.fit$arma[1], mean(tsb), tsb)
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;bootstrapping-the-statistic&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Bootstrapping the statistic&lt;/h1&gt;
&lt;p&gt;For time series data we will use the &lt;code&gt;tsboot&lt;/code&gt; function, which has several methods for resampling the time series data. We will show three of them here. I would refer the user to &lt;a href=&#34;http://www.stat.cmu.edu/~cshalizi/uADA/12/lectures/ch26.pdf&#34;&gt;this description&lt;/a&gt; for a further explanation on the difference between the methods. I’ll first set that we want to obtain a statistic for 100 resamplings of our data, and that we want our blocks for block sampling to be of the length of the data raised to the (1/3) power.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;num_resamples &amp;lt;- 1000
block_length &amp;lt;- round(length(manaus)^(1/3))&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;fixed-block-sampling&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Fixed Block Sampling&lt;/h2&gt;
&lt;p&gt;For fixed block sampling, you first generate a date to begin a block, and then select from that date to a point that gives a time series of length &lt;code&gt;block_length&lt;/code&gt;. You then draw a new time point, select a new block, and add it to your time series. You repeat the process until you have a time series of the length of your original time series. You then calculate a statistic for each sample, and aggregate the results.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;tsboot()&lt;/code&gt; function takes the original time series, your time series function that calculates the statistic (&lt;code&gt;manaus_fun&lt;/code&gt;), the number of samples you want (&lt;code&gt;R&lt;/code&gt;), the block length (&lt;code&gt;l&lt;/code&gt;), and the simulation type desired (&lt;code&gt;sim&lt;/code&gt;). In this case we will set &lt;code&gt;sim=&amp;quot;fixed&amp;quot;&lt;/code&gt; for fixed block sampling.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# the fixed block bootstrap with length
manaus_fixed &amp;lt;- tsboot(manaus, manaus_fun, R = num_resamples, l = block_length, 
    sim = &amp;quot;fixed&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;stationary-block-sampling&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Stationary Block Sampling&lt;/h2&gt;
&lt;p&gt;For stationary block sampling, you first generate a date to begin a block, and then select a random length generated by a geometric distribution with mean, &lt;code&gt;block_length&lt;/code&gt;. You save that block of data, and then draw a new time point and block length, select it, add it to your time series. You repeat the process until you have a time series of the length of your original time series. Just like in the fixed block sampling, you repeat that process until you have all of your resampled time series, and aggregate the statistics for each one.&lt;/p&gt;
&lt;p&gt;In this case we will set &lt;code&gt;sim=&amp;quot;geom&amp;quot;&lt;/code&gt; for stationary block sampling.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# the stationary bootstrap with mean block length 20
manaus_geom &amp;lt;- tsboot(manaus, manaus_fun, R = num_resamples, l = block_length, 
    sim = &amp;quot;geom&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;model-based-sampling&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model-based Sampling&lt;/h2&gt;
&lt;p&gt;For model-based resampling, the trick is to use your model to generate new time series, and adding noise with the resampled residuals from the fit. This method is particularly risky if you are not entirely sure about the underlying model for your data.&lt;/p&gt;
&lt;p&gt;We first have to fit the model, and extract the residuals from the model. Then we create a function that can simulate arima models according to our fit model. The &lt;code&gt;rand.gen&lt;/code&gt; argument to the arima.sim function is the most important for determining the resampling of the residuals, as the new innovations for simulating time series.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## First fit the model
manaus_ar &amp;lt;- auto.arima(manaus, max.p = 25, max.d = 0, max.q = 0, max.P = 0, 
    max.Q = 0, max.D = 0, ic = &amp;quot;aic&amp;quot;, max.order = 25, seasonal = FALSE)

## Create list containing the components of fit model
manaus_mod &amp;lt;- list(order = c(manaus_ar$arma[1], 0, 0), ar = coef(manaus_ar))

## Extract the residuals
manaus_res &amp;lt;- resid(manaus_ar) - mean(resid(manaus_ar))

## Simulation function for simulating
manaus_sim &amp;lt;- function(res, n.sim, ran.args) {
    # random generation of replicate series using arima.sim
    rg1 &amp;lt;- function(n, res) sample(res, n, replace = TRUE)
    ts.orig &amp;lt;- ran.args$ts
    ts.mod &amp;lt;- ran.args$model
    mean(ts.orig) + ts(arima.sim(model = ts.mod, n = n.sim, rand.gen = rg1, 
        res = as.vector(res)))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once we’ve setup the time-series simulation framework, we can once again use the &lt;code&gt;tsboot&lt;/code&gt; function with a few added arguments.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;manaus_model &amp;lt;- tsboot(manaus_res, manaus_fun, R = num_resamples, sim = &amp;quot;model&amp;quot;, 
    n.sim = length(manaus), orig.t = FALSE, ran.gen = manaus_sim, ran.args = list(ts = manaus, 
        model = manaus_mod))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;analyzing-the-results&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Analyzing the results&lt;/h1&gt;
&lt;p&gt;So we’ve bootstrapped the &lt;code&gt;manaus&lt;/code&gt; data three different ways to get an idea for the variability in AR model order (p). Let’s take a look at some of our simulated data first.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;recording_dates &amp;lt;- expand.grid(seq(1, 12), seq(1903, 1992))
recording_dates &amp;lt;- paste0(&amp;quot;1/&amp;quot;, recording_dates$Var1, &amp;quot;/&amp;quot;, recording_dates$Var2)

samp_data &amp;lt;- data_frame(time = dmy(recording_dates), true = c(manaus), fixed = manaus_fixed$t[1, 
    -c(1, 2)], stationary = manaus_geom$t[1, -c(1, 2)], model_based = manaus_model$t[1, 
    -c(1, 2)])

samp_data %&amp;gt;% gather(key, value, true:model_based) %&amp;gt;% ggplot(aes(time, value)) + 
    geom_line() + facet_wrap(~key)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://sjfox.github.io/post/2015-07-23-r-rmarkdown_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The data look very similar to the true data, and if we didn’t know which was true, we probably wouldn’t be able to tell the difference between them, which is exactly what we’d like. Now let’s take a look at the distributions for the order parameter of the AR model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p_dists &amp;lt;- data_frame(fixed = manaus_fixed$t[, 1], stationary = manaus_geom$t[, 
    1], model_based = manaus_model$t[, 1])

p_dists %&amp;gt;% gather(key, value, fixed:model_based) %&amp;gt;% ggplot(aes(value)) + geom_histogram(bins = 15) + 
    facet_wrap(~key, nrow = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://sjfox.github.io/post/2015-07-23-r-rmarkdown_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Looks like the stationary and fixed methods give similar distributions for the order of the autoregressive model, but the model-based distribution is slightly larger. The best fit model to the original data have an order of 4, so you can see that the model-based method for resampling maintains that level of dependence between the data. However, the stationary and fixed methods reduce the dependence between the data slightly (by combining chunks of the time series at random), so the best fit order for the AR models is reduced. Overall, the model-based method appears to work best in this scenario (though there is an increased risk of model misspecification), and we get a confidence interval from 3 to 5 for the order of the model.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>